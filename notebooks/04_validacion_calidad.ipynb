{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Validaci√≥n de Calidad de Datos\n",
                "\n",
                "Este notebook valida la calidad del dataset estandarizado y genera reportes finales.\n",
                "\n",
                "## Objetivos\n",
                "1. Generar reporte completo de calidad de datos\n",
                "2. Validar rangos y valores l√≥gicos\n",
                "3. Detectar outliers\n",
                "4. Validar columnas cr√≠ticas\n",
                "5. Documentar transformaciones aplicadas\n",
                "6. Generar m√©tricas finales del dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Importar librer√≠as\n",
                "import sys\n",
                "sys.path.append('../src')\n",
                "\n",
                "import polars as pl\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from etl.data_loader import load_from_parquet\n",
                "from etl.validators import (\n",
                "    generate_quality_report,\n",
                "    validate_critical_columns,\n",
                "    detect_outliers,\n",
                "    validate_ranges\n",
                ")\n",
                "from utils.config import STANDARDIZED_PARQUET_FILE, VALID_RANGES\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Cargar Datos Estandarizados"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\carlo\\Documents\\KrozFu\\repos\\fraud-detection-realestate\\notebooks\\..\\data\\processed\\igac_standardized.parquet\n",
                        "Existe? False\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "print(STANDARDIZED_PARQUET_FILE)\n",
                "print(\"Existe?\", os.path.exists(STANDARDIZED_PARQUET_FILE))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cargando datos desde Parquet: c:\\Users\\carlo\\Documents\\KrozFu\\repos\\fraud-detection-realestate\\notebooks\\..\\data\\processed\\igac_standardized.parquet\n"
                    ]
                },
                {
                    "ename": "FileNotFoundError",
                    "evalue": "The system cannot find the file specified. (os error 2): ...u\\repos\\fraud-detection-realestate\\notebooks\\..\\data\\processed\\igac_standardized.parquet (set POLARS_VERBOSE=1 to see full path)\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cargar dataset estandarizado\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mload_from_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTANDARDIZED_PARQUET_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_polars\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset cargado: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m filas, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columnas\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\Documents\\KrozFu\\repos\\fraud-detection-realestate\\notebooks\\../src\\etl\\data_loader.py:196\u001b[39m, in \u001b[36mload_from_parquet\u001b[39m\u001b[34m(file_path, use_polars)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCargando datos desde Parquet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_polars:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     df = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    198\u001b[39m     df = pd.read_parquet(file_path)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\Documents\\KrozFu\\repos\\fraud-detection-realestate\\venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\Documents\\KrozFu\\repos\\fraud-detection-realestate\\venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\Documents\\KrozFu\\repos\\fraud-detection-realestate\\venv\\Lib\\site-packages\\polars\\io\\parquet\\functions.py:289\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(source, columns, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, storage_options, credential_provider, retries, use_pyarrow, pyarrow_options, memory_map, include_file_paths, missing_columns, allow_missing_columns)\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    287\u001b[39m         lf = lf.select(columns)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\Documents\\KrozFu\\repos\\fraud-detection-realestate\\venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\Documents\\KrozFu\\repos\\fraud-detection-realestate\\venv\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:328\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    327\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\Documents\\KrozFu\\repos\\fraud-detection-realestate\\venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2422\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2420\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2421\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
                        "\u001b[31mFileNotFoundError\u001b[39m: The system cannot find the file specified. (os error 2): ...u\\repos\\fraud-detection-realestate\\notebooks\\..\\data\\processed\\igac_standardized.parquet (set POLARS_VERBOSE=1 to see full path)\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n"
                    ]
                }
            ],
            "source": [
                "# Cargar dataset estandarizado\n",
                "df = load_from_parquet(STANDARDIZED_PARQUET_FILE, use_polars=True)\n",
                "print(f\"Dataset cargado: {df.shape[0]:,} filas, {df.shape[1]} columnas\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Reporte de Calidad General"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generar reporte completo de calidad\n",
                "quality_report = generate_quality_report(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Validaci√≥n de Columnas Cr√≠ticas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validar que columnas cr√≠ticas no tengan nulos\n",
                "is_valid = validate_critical_columns(df)\n",
                "\n",
                "if is_valid:\n",
                "    print(\"\\n‚úÖ TODAS LAS COLUMNAS CR√çTICAS SON V√ÅLIDAS\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è ALGUNAS COLUMNAS CR√çTICAS TIENEN PROBLEMAS\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Validaci√≥n de Rangos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validar rangos de a√±os\n",
                "year_validation = validate_ranges(\n",
                "    df,\n",
                "    'YEAR_RADICA',\n",
                "    min_val=VALID_RANGES['YEAR_RADICA'][0],\n",
                "    max_val=VALID_RANGES['YEAR_RADICA'][1]\n",
                ")\n",
                "\n",
                "print(\"\\nValidaci√≥n de rangos de a√±os:\")\n",
                "for key, value in year_validation.items():\n",
                "    print(f\"  {key}: {value}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validar rangos de valores monetarios\n",
                "valor_validation = validate_ranges(\n",
                "    df,\n",
                "    'VALOR_AJUSTADO',\n",
                "    min_val=VALID_RANGES['VALOR'][0],\n",
                "    max_val=VALID_RANGES['VALOR'][1]\n",
                ")\n",
                "\n",
                "print(\"\\nValidaci√≥n de rangos de valores:\")\n",
                "for key, value in valor_validation.items():\n",
                "    print(f\"  {key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Detecci√≥n de Outliers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detectar outliers en valores ajustados usando m√©todo IQR\n",
                "df_with_outliers = detect_outliers(df, 'VALOR_AJUSTADO', method='iqr', threshold=1.5)\n",
                "\n",
                "# Contar outliers\n",
                "outlier_count = df_with_outliers['VALOR_AJUSTADO_OUTLIER'].sum()\n",
                "outlier_pct = (outlier_count / len(df)) * 100\n",
                "\n",
                "print(f\"\\nOutliers detectados: {outlier_count:,} ({outlier_pct:.2f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualizar distribuci√≥n con outliers marcados\n",
                "plt.figure(figsize=(14, 6))\n",
                "\n",
                "# Filtrar valores extremos para mejor visualizaci√≥n\n",
                "valores = df['VALOR_AJUSTADO'].filter(df['VALOR_AJUSTADO'].is_not_null())\n",
                "q99 = valores.quantile(0.99)\n",
                "valores_filtered = valores.filter(valores <= q99).to_list()\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.hist(valores_filtered, bins=50, edgecolor='black', alpha=0.7)\n",
                "plt.xlabel('Valor Ajustado (hasta percentil 99)')\n",
                "plt.ylabel('Frecuencia')\n",
                "plt.title('Distribuci√≥n de Valores Ajustados')\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.boxplot(valores_filtered)\n",
                "plt.ylabel('Valor Ajustado')\n",
                "plt.title('Boxplot de Valores Ajustados')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. An√°lisis de Completitud por Columna"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calcular porcentaje de completitud por columna\n",
                "completeness = []\n",
                "for col in df.columns:\n",
                "    null_count = df[col].null_count()\n",
                "    completeness_pct = ((len(df) - null_count) / len(df)) * 100\n",
                "    completeness.append({\n",
                "        'Columna': col,\n",
                "        'Completitud_%': round(completeness_pct, 2)\n",
                "    })\n",
                "\n",
                "completeness_df = pl.DataFrame(completeness).sort('Completitud_%')\n",
                "\n",
                "print(\"\\nCompletitud por columna (ordenado de menor a mayor):\")\n",
                "print(completeness_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualizar completitud\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.barh(completeness_df['Columna'].to_list(), completeness_df['Completitud_%'].to_list())\n",
                "plt.xlabel('Completitud (%)')\n",
                "plt.title('Completitud de Datos por Columna')\n",
                "plt.axvline(x=95, color='r', linestyle='--', label='95% threshold')\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. M√©tricas Finales del Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Resumen ejecutivo del dataset\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"RESUMEN EJECUTIVO DEL DATASET FINAL\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "print(f\"\\nüìä Dimensiones:\")\n",
                "print(f\"   - Total de registros: {len(df):,}\")\n",
                "print(f\"   - Total de columnas: {len(df.columns)}\")\n",
                "\n",
                "print(f\"\\nüìÖ Rango Temporal:\")\n",
                "print(f\"   - A√±o m√≠nimo: {df['YEAR_RADICA'].min()}\")\n",
                "print(f\"   - A√±o m√°ximo: {df['YEAR_RADICA'].max()}\")\n",
                "\n",
                "print(f\"\\nüåç Cobertura Geogr√°fica:\")\n",
                "print(f\"   - Departamentos √∫nicos: {df['DEPARTAMENTO'].n_unique()}\")\n",
                "print(f\"   - Municipios √∫nicos: {df['MUNICIPIO'].n_unique()}\")\n",
                "\n",
                "print(f\"\\nüí∞ Valores Monetarios (Ajustados):\")\n",
                "print(f\"   - Valor m√≠nimo: ${df['VALOR_AJUSTADO'].min():,.0f}\")\n",
                "print(f\"   - Valor m√°ximo: ${df['VALOR_AJUSTADO'].max():,.0f}\")\n",
                "print(f\"   - Valor promedio: ${df['VALOR_AJUSTADO'].mean():,.0f}\")\n",
                "print(f\"   - Valor mediano: ${df['VALOR_AJUSTADO'].median():,.0f}\")\n",
                "\n",
                "print(f\"\\n‚úÖ Calidad de Datos:\")\n",
                "total_nulls = sum([df[col].null_count() for col in df.columns])\n",
                "total_cells = len(df) * len(df.columns)\n",
                "completeness_overall = ((total_cells - total_nulls) / total_cells) * 100\n",
                "print(f\"   - Completitud general: {completeness_overall:.2f}%\")\n",
                "print(f\"   - Duplicados: 0 (eliminados en limpieza)\")\n",
                "print(f\"   - Outliers detectados: {outlier_count:,} ({outlier_pct:.2f}%)\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Documentaci√≥n de Transformaciones"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Documentar transformaciones aplicadas\n",
                "transformations = \"\"\"\n",
                "TRANSFORMACIONES APLICADAS AL DATASET\n",
                "======================================\n",
                "\n",
                "1. LIMPIEZA DE DATOS:\n",
                "   - Normalizaci√≥n de nombres de municipios y departamentos\n",
                "   - Parseo de fechas a formato datetime\n",
                "   - Limpieza de valores num√©ricos\n",
                "   - Eliminaci√≥n de duplicados basado en PK\n",
                "   - Manejo de valores nulos en columnas cr√≠ticas\n",
                "\n",
                "2. ESTANDARIZACI√ìN:\n",
                "   - Ajuste de valores monetarios por IPC (a√±o base: 2024)\n",
                "   - Normalizaci√≥n de c√≥digos DIVIPOLA a 5 d√≠gitos\n",
                "   - Creaci√≥n de campos temporales derivados:\n",
                "     * MES_RADICA\n",
                "     * TRIMESTRE_RADICA\n",
                "     * SEMESTRE_RADICA\n",
                "     * DIA_SEMANA_RADICA\n",
                "   - Creaci√≥n de clave geogr√°fica (GEO_KEY)\n",
                "   - C√°lculo de indicador de alto valor (ALTO_VALOR)\n",
                "\n",
                "3. VALIDACI√ìN:\n",
                "   - Validaci√≥n de rangos de a√±os (2015-2025)\n",
                "   - Validaci√≥n de valores monetarios\n",
                "   - Detecci√≥n de outliers usando m√©todo IQR\n",
                "   - Verificaci√≥n de completitud de datos\n",
                "\"\"\"\n",
                "\n",
                "print(transformations)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusiones\n",
                "\n",
                "### ‚úÖ Dataset Validado y Listo para An√°lisis\n",
                "\n",
                "El dataset ha pasado por un proceso completo de ETL:\n",
                "\n",
                "1. **Exploraci√≥n**: An√°lisis inicial de estructura y caracter√≠sticas\n",
                "2. **Limpieza**: Normalizaci√≥n, eliminaci√≥n de duplicados, manejo de nulos\n",
                "3. **Estandarizaci√≥n**: Ajuste por inflaci√≥n, creaci√≥n de campos derivados\n",
                "4. **Validaci√≥n**: Verificaci√≥n de calidad y detecci√≥n de anomal√≠as\n",
                "\n",
                "### üìà Pr√≥ximos Pasos\n",
                "\n",
                "1. **Fase 3**: Definici√≥n de la \"Normalidad\"\n",
                "   - An√°lisis estad√≠stico por municipio\n",
                "   - C√°lculo de m√©tricas base (promedio m¬≤, desviaci√≥n est√°ndar)\n",
                "   - Identificaci√≥n de patrones estacionales\n",
                "\n",
                "2. **Fase 4**: Detecci√≥n de Anomal√≠as\n",
                "   - Dise√±o de modelos de detecci√≥n\n",
                "   - Implementaci√≥n de reglas de negocio\n",
                "   - Validaci√≥n de resultados\n",
                "\n",
                "3. **Fase 5**: Visualizaci√≥n y Monitoreo\n",
                "   - Dashboard interactivo\n",
                "   - Sistema de alertas\n",
                "   - Reportes automatizados"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv (3.14.0)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
